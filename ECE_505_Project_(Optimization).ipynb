{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkE8pllqyZivNGjUO2ylnW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoVal1/ECE_505_Project-Optimization-/blob/main/ECE_505_Project_(Optimization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "I77sORtVrHcP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class unconstrained_optimizer:\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    \n",
        "    #function is the mathematical function that needs to be optimized\n",
        "    #gradient is the gadient of the mathematical function.\n",
        "    #line_search is method of finding alpha\n",
        "    def Steepest_Gradient_Descent_Method(self,function,gradient,line_search,\n",
        "                                         linear_first_derivative,condition_check,\n",
        "                                         epsilon,Iter_max,seed_x,linear_second_derivative=None,condition_param=0.5,seed_alpha=0.5,in_param=None,gamma_param=None):\n",
        "        #List to store values to be printed at each iteration\n",
        "        grad_value=[]\n",
        "        x_value=[]\n",
        "        alpha_value=[]\n",
        "        next_x=np.array(seed_x)\n",
        "        current_alpha_value=seed_alpha\n",
        "        for i in range(Iter_max):\n",
        "            #print(\"Iteration:\")\n",
        "            #print(i)\n",
        "            current_x=next_x\n",
        "            current_grad_value=gradient(current_x,in_param,gamma_param)\n",
        "            x_value.append(current_x)\n",
        "            grad_value.append(current_grad_value)\n",
        "            f_of_x=function(current_x,in_param,gamma_param)\n",
        "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.absolute(f_of_x))\n",
        "            \n",
        "            if criteria <= epsilon:\n",
        "                print(\"Criteria of epsilon met at i:\")\n",
        "                print(i)\n",
        "                return[x_value,grad_value,alpha_value,criteria]\n",
        "            if i==25 and condition_param<1e-3:\n",
        "                condition_param=0.8\n",
        "                alpha=0.9\n",
        "            elif i==50 and condition_param<1e-3:\n",
        "                condition_param=0.8\n",
        "                alpha=0.9\n",
        "            elif i==100 and condition_param<1e-2:\n",
        "                condition_param=0.8\n",
        "                alpha=0.9\n",
        "            elif i==200 and condition_param<1e-1:\n",
        "                condition_param=0.8\n",
        "                alpha=0.9\n",
        "            elif i==300:\n",
        "                alpha=1.5\n",
        "            elif i==500 :\n",
        "                condition_param=0.8\n",
        "                alpha=1.5\n",
        "            elif i==700:\n",
        "                alpha=1.5\n",
        "            \n",
        "            p=-current_grad_value\n",
        "            current_alpha_value=line_search(current_x,p,current_alpha_value,function,current_grad_value, \n",
        "                                            linear_first_derivative,linear_second_derivative,condition_check,\n",
        "                                            (condition_param),in_param,gamma_param)\n",
        "            alpha_value.append(current_alpha_value)\n",
        "            next_x=current_x+(current_alpha_value*p)\n",
        "        \n",
        "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
        "        current_x=next_x\n",
        "        current_grad_value=gradient(current_x,in_param,gamma_param)\n",
        "        x_value.append(current_x)\n",
        "        grad_value.append(current_grad_value)\n",
        "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))        \n",
        "        return[x_value,grad_value,alpha_value,criteria]\n",
        "    \n",
        "    \n",
        "    def Newtons_Method(self,function,gradient_function,hessian_function,epsilon,Iter_max,seed_x,in_param=None,gamma_param=None):\n",
        "        x_value=[]\n",
        "        grad_value=[]\n",
        "        direction_value=[]\n",
        "        next_x=np.array(seed_x)\n",
        "        for i in range(Iter_max):\n",
        "            #print(\"Iteration :\")\n",
        "            #print(i)\n",
        "            current_x_value=next_x\n",
        "            current_grad_value=gradient_function(current_x_value,in_param,gamma_param)\n",
        "            f_of_x=function(current_x_value,in_param,gamma_param)\n",
        "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.absolute(f_of_x))\n",
        "            x_value.append(current_x_value)\n",
        "            grad_value.append(current_grad_value)\n",
        "            if criteria <= epsilon:\n",
        "                print(\"Criteria of epsilon met at i:\")\n",
        "                print(i)\n",
        "                return[x_value,grad_value,direction_value,criteria]\n",
        "            current_hessian_value=hessian_function(current_x_value,in_param,gamma_param)\n",
        "            if np.linalg.det(current_hessian_value)<1e-5:\n",
        "                print(\"Warning: Hessian Matrix is Singluar for the current estimate of X!!!!!\")\n",
        "            \n",
        "            current_direction_value=np.linalg.lstsq(current_hessian_value,current_grad_value)[0]\n",
        "            direction_value.append(current_direction_value)\n",
        "            current_hessian_inv=np.linalg.pinv(current_hessian_value)\n",
        "            next_x=current_x_value-current_hessian_inv.dot(current_direction_value)\n",
        "            \n",
        "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
        "        current_x_value=next_x\n",
        "        current_grad_value=gradient_function(current_x_value,in_param,gamma_param)\n",
        "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
        "        x_value.append(current_x_value)\n",
        "        grad_value.append(current_grad_value)\n",
        "        return[x_value,grad_value,direction_value,criteria]\n",
        "    \n",
        "    def BFGS_Quasi_Newton_Method(self,function,gradient_function,line_search,\n",
        "                                 linear_first_derivative,condition_check,\n",
        "                                 epsilon,Iter_max,seed_Hessian,seed_x,\n",
        "                                 linear_second_derivative=None,condition_param=0.5,seed_alpha=0.5,in_param=None,gamma_param=None):\n",
        "        x_value=[]\n",
        "        grad_value=[]\n",
        "        direction_value=[]\n",
        "        next_x=np.matrix(seed_x).T\n",
        "        next_B=np.matrix(seed_Hessian)\n",
        "        next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
        "        alpha=seed_alpha\n",
        "        for i in range(Iter_max):\n",
        "            current_x_value=next_x\n",
        "            current_B=next_B\n",
        "            current_grad_value=next_gradient\n",
        "            x_value.append(current_x_value)\n",
        "            grad_value.append(current_grad_value)\n",
        "            f_of_x=function(current_x_value,in_param,gamma_param)\n",
        "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.absolute(f_of_x))\n",
        "            if criteria <= epsilon:\n",
        "                print(\"Criteria of epsilon met at i:\")\n",
        "                print(i)\n",
        "                return[x_value,grad_value,direction_value,criteria]\n",
        "            if np.linalg.det(current_B)<1e-5:\n",
        "                print(\"Warning:B matrix is Singluar for the current estimate of X!!!!!\")\n",
        "            \n",
        "            current_direction_value=np.linalg.lstsq(np.squeeze(np.asarray(current_B)),-1*np.squeeze(np.asarray(current_grad_value)))[0]\n",
        "            current_direction_value=np.matrix(current_direction_value).T\n",
        "            direction_value.append(current_direction_value)\n",
        "            alpha=line_search(current_x_value,current_direction_value,alpha,function,current_grad_value, \n",
        "                                            linear_first_derivative,linear_second_derivative,condition_check,\n",
        "                                            condition_param,in_param,gamma_param)\n",
        "            next_x=current_x_value+(alpha*current_direction_value)\n",
        "            next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
        "            s_k=next_x-current_x_value\n",
        "            y_k=next_gradient-current_grad_value\n",
        "            next_B=current_B-((((current_B*s_k)*(current_B*s_k).T)/(s_k.T*current_B*s_k)))+((y_k*y_k.T)/(y_k.T*s_k))\n",
        "            \n",
        "            \n",
        "        \n",
        "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
        "        current_x_value=next_x\n",
        "        current_grad_value=gradient_function(current_x_value,in_param,gamma_param)\n",
        "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
        "        x_value.append(current_x_value)\n",
        "        grad_value.append(current_grad_value)\n",
        "        return[x_value,grad_value,direction_value,criteria]\n",
        "     \n",
        "\n",
        "    def Conjugate_Gradient_Descent_Method(self,function,gradient_function,hessian_function,\n",
        "                                          epsilon,Iter_max,seed_x,in_param=None,gamma_param=None):\n",
        "        x_value=[]\n",
        "        grad_value=[]\n",
        "        direction_value=[]\n",
        "        next_x=np.matrix(seed_x).T\n",
        "        next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
        "        next_direction=-1*next_gradient\n",
        "        for i in range(Iter_max):\n",
        "            current_x=next_x\n",
        "            current_gradient=next_gradient\n",
        "            current_direction=next_direction\n",
        "            x_value.append(current_x)\n",
        "            grad_value.append(current_gradient)\n",
        "            direction_value.append(current_direction)\n",
        "            f_of_x=function(current_x,in_param,gamma_param)\n",
        "            criteria=np.linalg.norm(current_gradient,2)/(1+np.absolute(f_of_x))\n",
        "            \n",
        "            if criteria <= epsilon:\n",
        "                print(\"Criteria of epsilon met at i:\")\n",
        "                print(i)\n",
        "                return[x_value,grad_value,direction_value,criteria]\n",
        "           \n",
        "            alpha=-1*np.asscalar((current_direction.T*current_gradient)/(current_direction.T*np.matrix(hessian_function(current_x,in_param,gamma_param))*current_direction))\n",
        "            next_x=current_x+alpha*current_direction\n",
        "            next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
        "            beta=np.asscalar((current_direction.T*next_gradient)/(current_direction.T*np.matrix(hessian_function(next_x,in_param,gamma_param))*current_direction))\n",
        "            next_direction=-1*next_gradient+(beta*current_direction)\n",
        "        \n",
        "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
        "        current_x=next_x\n",
        "        current_gradient=next_gradient\n",
        "        current_direction=next_direction\n",
        "        criteria=np.linalg.norm(current_gradient,2)/(1+np.linalg.norm(current_gradient,1))\n",
        "        x_value.append(current_x)\n",
        "        grad_value.append(current_gradient)\n",
        "        direction_value.append(current_direction)\n",
        "        return[x_value,grad_value,direction_value,criteria]\n",
        "    \n",
        "\n",
        "def Newton_line_search(x,p,alpha,function,grad_val,linear_first_derivative,linear_second_derivative,condition_check,condition_param,in_param,gamma_param):\n",
        "    next_alpha=alpha\n",
        "    condition=condition_check(condition_param,function,grad_val,x,p,next_alpha,in_param,gamma_param)\n",
        "    while(condition==False):\n",
        "        current_alpha=next_alpha\n",
        "        first_deri=linear_first_derivative(current_alpha,x,p,in_param,gamma_param)\n",
        "        second_deri=linear_second_derivative(current_alpha,x,p,in_param,gamma_param)\n",
        "        next_alpha=current_alpha-(first_deri/second_deri)\n",
        "        condition=condition_check(condition_param,function,grad_val,x,p,next_alpha,in_param,gamma_param)\n",
        "        \n",
        "    return next_alpha\n",
        "\n",
        "#def secant_line_search(x,p,alpha,function,linear_first_derivative,linear_second_derivative,condition_check,condition_param):\n",
        "#    condition=False\n",
        "#    next_alpha=alpha\n",
        "#    while(condition==False):\n",
        "#        current_alpha=next_alpha\n",
        "#        first_deri=linear_first_derivative(current_alpha,x,p)\n",
        "#        second_deri=linear_second_derivative(current_alpha,x,p)\n",
        "#        next_alpha=current_alpha-(first_deri/second_deri)\n",
        "#        condition=condition_check(condition_param,function,x,p,next_alpha)\n",
        "#        \n",
        "#    return next_alpha   \n",
        "\n",
        "#def Wolfe_Condition(condition_param,function,x,p,next_alpha):\n",
        "#    pass\n",
        "\n",
        "def Armijo_Condition(condition_param,function,grad_val,x,p,alpha,in_param,gamma_param):\n",
        "    x=np.squeeze(np.asarray(x))\n",
        "    p=np.squeeze(np.asarray(p))\n",
        "    grad_val=np.squeeze(np.asarray(grad_val))\n",
        "    LHS=function(x+alpha*p,in_param,gamma_param)\n",
        "    RHS=function(x,in_param,gamma_param)+(condition_param*alpha*np.dot(p,grad_val))\n",
        "    if LHS<=RHS:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    "
      ],
      "metadata": {
        "id": "0Vq-NZuvufjI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_pdf(inp,mean,variance):\n",
        "    return (1/(2*np.pi*np.sqrt(variance)))*np.exp(-1*((inp-mean)**2/(2*variance)))\n",
        "\n",
        "def marginal_log_likelihood_funtion_x(x,theta):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    x=np.squeeze(np.asarray(x))\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=theta[0]\n",
        "        mu2=theta[2]\n",
        "        v1=theta[1]\n",
        "        v2=theta[3]\n",
        "        P1=theta[4]\n",
        "        P2=1-theta[4]\n",
        "        gauss1=gaussian_pdf(x,mu1,v1)\n",
        "        gauss2=gaussian_pdf(x,mu2,v2)\n",
        "        return np.sum(np.log(P1*gauss1+P2*gauss2))\n",
        "\n",
        "def Expection_function(x,theta):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    x=np.squeeze(np.asarray(x))\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=theta[0]\n",
        "        mu2=theta[2]\n",
        "        v1=theta[1]\n",
        "        v2=theta[3]\n",
        "        P=[theta[4],1-theta[4]]\n",
        "        gauss=[gaussian_pdf(x,mu1,v1),gaussian_pdf(x,mu2,v2)]\n",
        "        denominator=P[0]*gauss[0]+P[1]*gauss[1]\n",
        "        return [P[0]*gauss[0]/denominator,P[1]*gauss[1]/denominator]\n",
        "\n",
        "def Q_function(theta,x,gamma):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    x=np.squeeze(np.asarray(x))\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=theta[0]\n",
        "        mu2=theta[2]\n",
        "        v1=theta[1]\n",
        "        v2=theta[3]\n",
        "        P1=theta[4]\n",
        "        P2=1-theta[4]\n",
        "        gauss1=gaussian_pdf(x,mu1,v1)\n",
        "        gauss2=gaussian_pdf(x,mu2,v2)\n",
        "        Q=np.log(P1*gauss1)*gamma[0]+np.log(P2*gauss2)*gamma[1]\n",
        "        Q=-1*np.sum(Q)\n",
        "        return Q\n",
        "        \n",
        "def grad_Q_function(theta,x,gamma):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    x=np.squeeze(np.asarray(x))\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=theta[0]\n",
        "        mu2=theta[2]\n",
        "        v1=theta[1]\n",
        "        v2=theta[3]\n",
        "        P1=theta[4]\n",
        "        P2=1-theta[4]\n",
        "        dq_dmu1=-1*np.sum(((x-mu1)/v1)*gamma[0])\n",
        "        dq_dv1=-1*np.sum(((-0.5/v1)+(0.5*(x-mu1)**2/v1**2))*gamma[0])\n",
        "        dq_dmu2=-1*np.sum(((x-mu2)/v2)*gamma[1])\n",
        "        dq_dv2=-1*np.sum(((-0.5/v2)+(0.5*(x-mu2)**2/v2**2))*gamma[1])\n",
        "        dq_dp=-1*np.sum((1/P1)*gamma[0]-(1/P2)*gamma[1])\n",
        "        return np.array([dq_dmu1,dq_dv1,dq_dmu2,dq_dv2,dq_dp])\n",
        "\n",
        "def hessian_Q_function(theta,x,gamma):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    x=np.squeeze(np.asarray(x))\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=theta[0]\n",
        "        mu2=theta[2]\n",
        "        v1=theta[1]\n",
        "        v2=theta[3]\n",
        "        P1=theta[4]\n",
        "        P2=1-theta[4]\n",
        "        d2q_dmu1=-1*np.sum(((-1/v1)*gamma[0]))\n",
        "        d2q_dmu1v1=-1*np.sum((-1*(x-mu1)/v1**2)*gamma[0])\n",
        "        d2q_dv1=-1*np.sum(((0.5/v1**2)+(-1*(x-mu1)**2/v1**3))*gamma[0])\n",
        "        d2q_dmu2=-1*np.sum((-1/v2)*gamma[1])\n",
        "        d2q_dmu2v2=-1*np.sum((-1*(x-mu2)/v2**2)*gamma[1])\n",
        "        d2q_dv2=-1*np.sum(((0.5/v2**2)+(-1*(x-mu2)**2/v2**3))*gamma[1])\n",
        "        d2q_dp=-1*np.sum((-1/P1**2)*gamma[0]-(-1/P2**2)*gamma[1])\n",
        "        return np.array([[d2q_dmu1,d2q_dmu1v1,0,0,0],[d2q_dmu1v1,d2q_dv1,0,0,0],[0,0,d2q_dmu2,d2q_dmu2v2,0],[0,0,d2q_dmu2v2,d2q_dv2,0],[0,0,0,0,d2q_dp]])\n",
        "\n",
        "def linearfd_Q_func_alpha(alpha,theta,theta_p,x,gamma_param):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    theta_p=np.squeeze(np.asarray(theta_p))\n",
        "    nt=theta+alpha*theta_p\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=nt[0]\n",
        "        mu2=nt[2]\n",
        "        v1=nt[1]\n",
        "        v2=nt[3]\n",
        "        P1=nt[4]\n",
        "        P2=1-P1\n",
        "        rval=((theta_p[4]/P1)+(0.5*(theta_p[1]/v1))+(0.5*(v1*2*(x-mu1)*-1*theta_p[0]-(x-mu1)**2*theta_p[1])/v1**2))*gamma_param[0]\n",
        "        rval+=((-1*theta_p[4]/P2)+(0.5*(theta_p[3]/v2))+(0.5*(v2*2*(x-mu2)*-1*theta_p[2]-(x-mu2)**2*theta_p[3])/v2**2))*gamma_param[1]\n",
        "        rval=-1*np.sum(rval)\n",
        "        return rval\n",
        "        \n",
        "def linearsd_Q_func_alpha(alpha,theta,theta_p,x,gamma_param):\n",
        "    theta=np.squeeze(np.asarray(theta))\n",
        "    theta_p=np.squeeze(np.asarray(theta_p))\n",
        "    nt=theta+alpha*theta_p\n",
        "    if len(theta)!=5:\n",
        "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
        "    else:\n",
        "        mu1=nt[0]\n",
        "        mu2=nt[2]\n",
        "        v1=nt[1]\n",
        "        v2=nt[3]\n",
        "        P1=nt[4]\n",
        "        P2=1-P1\n",
        "        n1=v1*2*(x-mu1)*-1*theta_p[0]-(x-mu1)**2*theta_p[1]\n",
        "        n2=v2*2*(x-mu2)*-1*theta_p[2]-(x-mu2)**2*theta_p[3]\n",
        "        dn1=(v1*2*theta_p[0]**2)\n",
        "        dn2=(v2*2*theta_p[2]**2)\n",
        "        rval=((-1*theta_p[4]**2/P1**2)+(0.5*(-1*theta_p[1]**2/v1**2))+(-1*(v1**2*dn1-n1*2*v1*theta_p[1])/v1**3))*gamma_param[0]\n",
        "        rval+=((theta_p[4]**2/P2**2)+(0.5*(-1*theta_p[3]**2/v2**2))+(-1*(v2**2*dn2-n2*2*v2*theta_p[3])/v2**3))*gamma_param[1]\n",
        "        rval=-1*np.sum(rval)\n",
        "        return rval\n",
        "    \n"
      ],
      "metadata": {
        "id": "ZjDSKM6dqz0b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def E_M_Algorithm(file_name,seed_theta):\n",
        "    x=np.loadtxt(file_name,dtype=int,delimiter='\\n')\n",
        "    optimize=unconstrained_optimizer()\n",
        "    seed_theta=np.array(seed_theta)\n",
        "    inital_p_x_given_theta=marginal_log_likelihood_funtion_x(x,seed_theta)\n",
        "    \n",
        "    if len(seed_theta)!=5:\n",
        "        sys.exit(\"E_M can handle only vector of size 5\")\n",
        "   \n",
        "    for i in range(100):\n",
        "        # Expectation step\n",
        "        gamma=Expection_function(x,seed_theta)\n",
        "        \n",
        "        # Maximization Step/ minimization step\n",
        "            \n",
        "            \n",
        "        [new_theta,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(Q_function,grad_Q_function,\n",
        "                                                                         Newton_line_search,linearfd_Q_func_alpha,\n",
        "                                                                         Armijo_Condition,\n",
        "                                                                          1e-5,1000,seed_theta,linearsd_Q_func_alpha,\n",
        "                                                                          1e-4,1e-3,x,gamma)\n",
        "        \n",
        "        \n",
        "        new_p_x_given_theta=marginal_log_likelihood_funtion_x(x,new_theta[-1])\n",
        "        seed_theta=new_theta[-1]\n",
        "        c=np.sqrt((new_p_x_given_theta-inital_p_x_given_theta)**2)\n",
        "        if(c<=1e-5):\n",
        "            print(\"Convergence of Marginal probability has happened\")\n",
        "            return seed_theta\n",
        "        else:\n",
        "            inital_p_x_given_theta=new_p_x_given_theta\n",
        "    \n",
        "    print(\"Convergence of Marginal probability has not happened\")\n",
        "    return seed_theta\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "    \n",
        "theta=E_M_Algorithm(\"image_samples.txt\",[120.3091,185.1229,201.7558,201.440,0.61739])\n",
        "#theta=E_M_Algorithm(\"image_samples.txt\",[100,50,150,50,0.5])\n",
        "print(\"Mean1:\")\n",
        "print(theta[0])\n",
        "print(\"Var1:\")\n",
        "print(theta[1])\n",
        "print(\"Mean2:\")\n",
        "print(theta[2])\n",
        "print(\"Var2:\")\n",
        "print(theta[3])\n",
        "print(\"P:\")\n",
        "print(theta[4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-gBYorUq-Qf",
        "outputId": "f8831c92-36d6-46a4-c9fa-c039a598b0aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Maximum iteration reached.Criteria of epsilon not met\n",
            "Convergence of Marginal probability has not happened\n",
            "Mean1:\n",
            "120.26131051825456\n",
            "Var1:\n",
            "196.28040276252787\n",
            "Mean2:\n",
            "201.6590417843259\n",
            "Var2:\n",
            "218.19920850398717\n",
            "P:\n",
            "0.6165765961562276\n"
          ]
        }
      ]
    }
  ]
}